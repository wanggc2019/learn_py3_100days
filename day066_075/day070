2020/4/18
****解析动态内容****
--解析动态内容
网站其内容或部分内容是通过JavaScript动态生成的，这就意味着在浏览器窗口中“查看网页源代码”时无法在HTML代码中找到这些内容，也就是说我们之前用
的抓取数据的方式无法正常运转了。解决这样的问题基本上有两种方案，一是JavaScript逆向工程；另一种是渲染JavaScript获得渲染后的内容。
1、JavaScript逆向工程
找到通过Ajax技术动态获取数据的接口

2、使用Selenium
尽管很多网站对自己的网络API接口进行了保护，增加了获取数据的难度，但是只要经过足够的努力，绝大多数还是可以被逆向工程的，但是在实际开发中，我
们可以通过浏览器渲染引擎来避免这些繁琐的工作，WebKit就是一个利用的渲染引擎。



2020/4/19
****表单交互和验证码处理****
又咕了？



****Scrapy爬虫框架入门****
学习网站：
https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html
1、概念
Scrapy是Python开发的一个非常流行的网络爬虫框架，可以用来抓取Web站点并从页面中提取结构化的数据，被广泛的用于数据挖掘、数据监测和自动化测试等领域。

2、pycharm创建scrapy项目
1)检查是否安装scrapy，import scrapy，没有则安装：pip3 install scrapy
2)生成scrapy目录结构：
(venv) E:\PyProject\learn_py3_100days\day066_075>scrapy startproject tutorial
3）执行scrapy：到scrapy根目录下即tutorial执行：
(venv) E:\PyProject\learn_py3_100days\day066_075>cd tutorial
(venv) E:\PyProject\learn_py3_100days\day066_075\tutorial>scrapy crawl dmoz
注意：一定要到到scrapy跟目录下，否则会报错：Unknown command: crawl

3、scrapt组件介绍
组件
Scrapy引擎（Engine）：Scrapy引擎是用来控制整个系统的数据处理流程。
调度器（Scheduler）：调度器从Scrapy引擎接受请求并排序列入队列，并在Scrapy引擎发出请求后返还给它们。
下载器（Downloader）：下载器的主要职责是抓取网页并将网页内容返还给蜘蛛（Spiders）。
蜘蛛（Spiders）：蜘蛛是有Scrapy用户自定义的用来解析网页并抓取特定URL返回的内容的类，每个蜘蛛都能处理一个域名或一组域名，简单的说就是用来定义特定网站的抓取和解析规则。
条目管道（Item Pipeline）：条目管道的主要责任是负责处理有蜘蛛从网页中抽取的数据条目，它的主要任务是清理、验证和存储数据。当页面被蜘蛛解析后，将被发送到条目管道，并经过几个特定的次序处理数据。每个条目管道组件都是一个Python类，它们获取了数据条目并执行对数据条目进行处理的方法，同时还需要确定是否需要在条目管道中继续执行下一步或是直接丢弃掉不处理。条目管道通常执行的任务有：清理HTML数据、验证解析到的数据（检查条目是否包含必要的字段）、检查是不是重复数据（如果重复就丢弃）、将解析到的数据存储到数据库（关系型数据库或NoSQL数据库）中。
中间件（Middlewares）：中间件是介于Scrapy引擎和其他组件之间的一个钩子框架，主要是为了提供自定义的代码来拓展Scrapy的功能，包括下载器中间件和蜘蛛中间件。

4、数据处理流程
Scrapy的整个数据处理流程由Scrapy引擎进行控制，通常的运转流程包括以下的步骤：
引擎询问蜘蛛需要处理哪个网站，并让蜘蛛将第一个需要处理的URL交给它。
引擎让调度器将需要处理的URL放在队列中。
引擎从调度那获取接下来进行爬取的页面。
调度将下一个爬取的URL返回给引擎，引擎将它通过下载中间件发送到下载器。
当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎；如果下载失败了，引擎会通知调度器记录这个URL，待会再重新下载。
引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。
引擎将抓取到的数据条目送入条目管道，把新的URL发送给调度器放入队列中。
上述操作中的2-8步会一直重复直到调度器中没有需要请求的URL，爬虫停止工作。



